<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<p:notes xmlns:a="http://schemas.openxmlformats.org/drawingml/2006/main" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:p="http://schemas.openxmlformats.org/presentationml/2006/main" showMasterSp="0" showMasterPhAnim="0"><p:cSld><p:spTree><p:nvGrpSpPr><p:cNvPr id="1" name="Shape 286"/><p:cNvGrpSpPr/><p:nvPr/></p:nvGrpSpPr><p:grpSpPr><a:xfrm><a:off x="0" y="0"/><a:ext cx="0" cy="0"/><a:chOff x="0" y="0"/><a:chExt cx="0" cy="0"/></a:xfrm></p:grpSpPr><p:sp><p:nvSpPr><p:cNvPr id="287" name="Google Shape;287;gb248b7f6f2_2_343:notes"/><p:cNvSpPr><a:spLocks noGrp="1" noRot="1" noChangeAspect="1"/></p:cNvSpPr><p:nvPr><p:ph type="sldImg" idx="2"/></p:nvPr></p:nvSpPr><p:spPr><a:xfrm><a:off x="381300" y="685800"/><a:ext cx="6096000" cy="3429000"/></a:xfrm><a:custGeom><a:avLst/><a:gdLst/><a:ahLst/><a:cxnLst/><a:rect l="l" t="t" r="r" b="b"/><a:pathLst><a:path w="120000" h="120000" extrusionOk="0"><a:moveTo><a:pt x="0" y="0"/></a:moveTo><a:lnTo><a:pt x="120000" y="0"/></a:lnTo><a:lnTo><a:pt x="120000" y="120000"/></a:lnTo><a:lnTo><a:pt x="0" y="120000"/></a:lnTo><a:close/></a:path></a:pathLst></a:custGeom></p:spPr></p:sp><p:sp><p:nvSpPr><p:cNvPr id="288" name="Google Shape;288;gb248b7f6f2_2_343:notes"/><p:cNvSpPr txBox="1"><a:spLocks noGrp="1"/></p:cNvSpPr><p:nvPr><p:ph type="body" idx="1"/></p:nvPr></p:nvSpPr><p:spPr><a:xfrm><a:off x="685800" y="4343400"/><a:ext cx="5486400" cy="4114800"/></a:xfrm><a:prstGeom prst="rect"><a:avLst/></a:prstGeom></p:spPr><p:txBody><a:bodyPr spcFirstLastPara="1" wrap="square" lIns="91425" tIns="91425" rIns="91425" bIns="91425" anchor="t" anchorCtr="0"><a:noAutofit/></a:bodyPr><a:lstStyle/><a:p><a:pPr marL="0" lvl="0" indent="0" algn="l" rtl="0"><a:spcBef><a:spcPts val="0"/></a:spcBef><a:spcAft><a:spcPts val="0"/></a:spcAft><a:buClr><a:schemeClr val="dk1"/></a:buClr><a:buSzPts val="1100"/><a:buFont typeface="Arial"/><a:buNone/></a:pPr><a:r><a:rPr lang="en"><a:solidFill><a:schemeClr val="dk1"/></a:solidFill></a:rPr><a:t>And for the process of the action recognition, we first input the frame(meaning the images mentioned before), and then get the key points by openpose model. After that, we preprocess the key points, through normalizing them with the personâ€™s height, and obtaining velocity of action by comparing coordinates between frames. We next obtain the time-serial features using the speed, and normalized positions from multiple adjacent images. Next, we used PCA to reduce the feature dimension from 314 to 50. Then, we classify correct and incorrect actions by a DNN model with 3 layers of 20, 30 and 50 neurons on each layer. Finally, we output the label and skeleton to the output frame, and save the frames to videos, and show them on the result page. </a:t></a:r><a:endParaRPr><a:solidFill><a:schemeClr val="dk1"/></a:solidFill></a:endParaRPr></a:p><a:p><a:pPr marL="0" lvl="0" indent="0" algn="l" rtl="0"><a:spcBef><a:spcPts val="0"/></a:spcBef><a:spcAft><a:spcPts val="0"/></a:spcAft><a:buClr><a:schemeClr val="dk1"/></a:buClr><a:buSzPts val="1100"/><a:buFont typeface="Arial"/><a:buNone/></a:pPr><a:endParaRPr><a:solidFill><a:schemeClr val="dk1"/></a:solidFill></a:endParaRPr></a:p><a:p><a:pPr marL="0" lvl="0" indent="0" algn="l" rtl="0"><a:spcBef><a:spcPts val="0"/></a:spcBef><a:spcAft><a:spcPts val="0"/></a:spcAft><a:buClr><a:schemeClr val="dk1"/></a:buClr><a:buSzPts val="1100"/><a:buFont typeface="Arial"/><a:buNone/></a:pPr><a:r><a:rPr lang="en"><a:solidFill><a:schemeClr val="dk1"/></a:solidFill></a:rPr><a:t>also have mean filtering </a:t></a:r><a:r><a:rPr lang="en"><a:solidFill><a:srgbClr val="24292E"/></a:solidFill><a:highlight><a:schemeClr val="lt1"/></a:highlight></a:rPr><a:t>the prediction scores between 2 frames</a:t></a:r><a:endParaRPr/></a:p></p:txBody></p:sp></p:spTree></p:cSld><p:clrMapOvr><a:masterClrMapping/></p:clrMapOvr></p:notes>